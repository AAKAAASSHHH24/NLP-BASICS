{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaXSFHm3IVvalonC5L1SIs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAKAAASSHHH24/NLP-BASICS/blob/main/Generating_text_like_Shakespeare_using_keras_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "rugyugcdVva5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QOjzeAtGfIjM",
        "outputId": "281657e8-b5da-4e2c-a0b8-796d9f3019c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"data\", exist_ok = True)\n",
        "os.chdir ('data')"
      ],
      "metadata": {
        "id": "ZnSMO-CAVup3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt > \"shakespeare.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiPZtah_VuQU",
        "outputId": "84ab8317-b8b9-435f-e357-5072df253523"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 1089k  100 1089k    0     0  22.6M      0 --:--:-- --:--:-- --:--:-- 22.6M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bJT6rSBwUj0g"
      },
      "outputs": [],
      "source": [
        "#this can be done using config.yaml\n",
        "class Config:\n",
        "  path_to_file = os.path.join(\"/content/data\",\"shakespeare.txt\")\n",
        "  seq_length = 100  # AT ONE GO HOW MUCH IS THE LENGTH OF THE WORDS\n",
        "\n",
        "  batch_size = 64\n",
        "  buffer_size = 10000   # KEEP THIS MUCH DATA IN BUFFER OR RAM (pick randomly from this buffer FOR FASTER READ/WRITE DURING TRAINING)\n",
        "\n",
        "  embedding_dim =256\n",
        "\n",
        "  rnn_units = 1024\n",
        "\n",
        "  epochs = 30\n",
        "  checkpoint_dir = \"./training_ckpt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Config.path_to_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QQSXc3SxaoGL",
        "outputId": "fb2ba9ef-5bdf-490c-c882-e434693bf199"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/data/shakespeare.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(Config.path_to_file, \"rb\").read().decode(encoding = 'utf-8')\n",
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Jf5X3MnYXqEN",
        "outputId": "88a9a611-fa6e-443c-97bf-08c630f47f4e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#letters of the text unique ones\n",
        "sorted(set(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJcX7EtcYNrb",
        "outputId": "f80cd4f8-c0cf-4c55-aee6-88a274056bb8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '$',\n",
              " '&',\n",
              " \"'\",\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '3',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "characters =sorted(set(text))\n",
        "len(characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NHCY5gbfweN",
        "outputId": "6f6d4ecf-8a2f-4d73-f082-9dd73bd43d65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx2char = np.array(characters)\n",
        "idx2char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQqmTLfEt-2v",
        "outputId": "7d07a15e-3199-40dc-cfb4-df9753b7b770"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?',\n",
              "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
              "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
              "       'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
              "       'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'],\n",
              "      dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vectorization\n",
        "char_dict = {char:idx for idx,char in enumerate(characters)}\n",
        "char_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ8mRVV2gaAE",
        "outputId": "e3abf8c2-3f22-425a-8fd6-f78dd378a0f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '$': 3,\n",
              " '&': 4,\n",
              " \"'\": 5,\n",
              " ',': 6,\n",
              " '-': 7,\n",
              " '.': 8,\n",
              " '3': 9,\n",
              " ':': 10,\n",
              " ';': 11,\n",
              " '?': 12,\n",
              " 'A': 13,\n",
              " 'B': 14,\n",
              " 'C': 15,\n",
              " 'D': 16,\n",
              " 'E': 17,\n",
              " 'F': 18,\n",
              " 'G': 19,\n",
              " 'H': 20,\n",
              " 'I': 21,\n",
              " 'J': 22,\n",
              " 'K': 23,\n",
              " 'L': 24,\n",
              " 'M': 25,\n",
              " 'N': 26,\n",
              " 'O': 27,\n",
              " 'P': 28,\n",
              " 'Q': 29,\n",
              " 'R': 30,\n",
              " 'S': 31,\n",
              " 'T': 32,\n",
              " 'U': 33,\n",
              " 'V': 34,\n",
              " 'W': 35,\n",
              " 'X': 36,\n",
              " 'Y': 37,\n",
              " 'Z': 38,\n",
              " 'a': 39,\n",
              " 'b': 40,\n",
              " 'c': 41,\n",
              " 'd': 42,\n",
              " 'e': 43,\n",
              " 'f': 44,\n",
              " 'g': 45,\n",
              " 'h': 46,\n",
              " 'i': 47,\n",
              " 'j': 48,\n",
              " 'k': 49,\n",
              " 'l': 50,\n",
              " 'm': 51,\n",
              " 'n': 52,\n",
              " 'o': 53,\n",
              " 'p': 54,\n",
              " 'q': 55,\n",
              " 'r': 56,\n",
              " 's': 57,\n",
              " 't': 58,\n",
              " 'u': 59,\n",
              " 'v': 60,\n",
              " 'w': 61,\n",
              " 'x': 62,\n",
              " 'y': 63,\n",
              " 'z': 64}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#text_as_int = np.array(list(char_dict.values()))\n",
        "text_as_int = np.array([char_dict[c] for c in text])\n",
        "text_as_int"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyBy1YdYhBi2",
        "outputId": "a3deba0c-7654-43fa-956e-ddf727c1674a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([18, 47, 56, ..., 45,  8,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples_per_epoch = len(text)//(Config.seq_length + 1)\n",
        "examples_per_epoch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9y8VGs4G1ls",
        "outputId": "28153abc-af0a-44fc-ad61-22677825a80b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11043"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hi\\nthere\"), print(repr('hi\\nthere'))  #demonstrating use of repr()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbsqkriDPUOW",
        "outputId": "a68e2736-be4d-4834-9097-4a936d99ec00"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi\n",
            "there\n",
            "'hi\\nthere'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_2_char = {idx:char for idx,char in enumerate(characters)}\n",
        "idx_2_char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwrdKST0Mw0w",
        "outputId": "30c95dd9-5cb6-48a7-9c4e-c31c6583f2e6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '\\n',\n",
              " 1: ' ',\n",
              " 2: '!',\n",
              " 3: '$',\n",
              " 4: '&',\n",
              " 5: \"'\",\n",
              " 6: ',',\n",
              " 7: '-',\n",
              " 8: '.',\n",
              " 9: '3',\n",
              " 10: ':',\n",
              " 11: ';',\n",
              " 12: '?',\n",
              " 13: 'A',\n",
              " 14: 'B',\n",
              " 15: 'C',\n",
              " 16: 'D',\n",
              " 17: 'E',\n",
              " 18: 'F',\n",
              " 19: 'G',\n",
              " 20: 'H',\n",
              " 21: 'I',\n",
              " 22: 'J',\n",
              " 23: 'K',\n",
              " 24: 'L',\n",
              " 25: 'M',\n",
              " 26: 'N',\n",
              " 27: 'O',\n",
              " 28: 'P',\n",
              " 29: 'Q',\n",
              " 30: 'R',\n",
              " 31: 'S',\n",
              " 32: 'T',\n",
              " 33: 'U',\n",
              " 34: 'V',\n",
              " 35: 'W',\n",
              " 36: 'X',\n",
              " 37: 'Y',\n",
              " 38: 'Z',\n",
              " 39: 'a',\n",
              " 40: 'b',\n",
              " 41: 'c',\n",
              " 42: 'd',\n",
              " 43: 'e',\n",
              " 44: 'f',\n",
              " 45: 'g',\n",
              " 46: 'h',\n",
              " 47: 'i',\n",
              " 48: 'j',\n",
              " 49: 'k',\n",
              " 50: 'l',\n",
              " 51: 'm',\n",
              " 52: 'n',\n",
              " 53: 'o',\n",
              " 54: 'p',\n",
              " 55: 'q',\n",
              " 56: 'r',\n",
              " 57: 's',\n",
              " 58: 't',\n",
              " 59: 'u',\n",
              " 60: 'v',\n",
              " 61: 'w',\n",
              " 62: 'x',\n",
              " 63: 'y',\n",
              " 64: 'z'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_2_char[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3L-hYpShmNF_",
        "outputId": "67e3c2ff-6931-466f-976d-20ca32254ce0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_value = list(char_dict.values())\n",
        "char_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cA9qRxgmoEe",
        "outputId": "4c893c37-3fa5-4331-9f80-279d43d17c8d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 23,\n",
              " 24,\n",
              " 25,\n",
              " 26,\n",
              " 27,\n",
              " 28,\n",
              " 29,\n",
              " 30,\n",
              " 31,\n",
              " 32,\n",
              " 33,\n",
              " 34,\n",
              " 35,\n",
              " 36,\n",
              " 37,\n",
              " 38,\n",
              " 39,\n",
              " 40,\n",
              " 41,\n",
              " 42,\n",
              " 43,\n",
              " 44,\n",
              " 45,\n",
              " 46,\n",
              " 47,\n",
              " 48,\n",
              " 49,\n",
              " 50,\n",
              " 51,\n",
              " 52,\n",
              " 53,\n",
              " 54,\n",
              " 55,\n",
              " 56,\n",
              " 57,\n",
              " 58,\n",
              " 59,\n",
              " 60,\n",
              " 61,\n",
              " 62,\n",
              " 63,\n",
              " 64]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[:13]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vCOx7fMckf48",
        "outputId": "c0ad6a83-1af0-4ec2-cfa9-2fe039ead396"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a dataset from the above, every character is read is converted to dataset\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # API supports writing descriptive and efficient input pipelines.\n",
        "\n",
        "for i in char_dataset.take(20):\n",
        "  print(idx2char[i.numpy()])\n",
        "\n",
        "#print(char_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpyWWy3CQcpy",
        "outputId": "a4a68fe8-dc3a-467f-cb85-83d71b7926b9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n",
            "z\n",
            "e\n",
            "n\n",
            ":\n",
            "\n",
            "\n",
            "B\n",
            "e\n",
            "f\n",
            "o\n",
            "r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(Config.seq_length + 1, drop_remainder = True)  # creating batches of 100 and handling the remainder, passing an array as dataset\n",
        "#print(idx_2_char.values())\n",
        "for item in sequences.take(20):  # taking this many number of batches\n",
        "  to_print = repr(\"\".join([idx2char[c] for c in item.numpy()]))\n",
        "  print(to_print)\n",
        "  print(len(to_print))"
      ],
      "metadata": {
        "id": "1_oXLTuAXfrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09b48396-d44f-431e-a1d8-d992a5b5c378"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "110\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "109\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "109\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "107\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n",
            "109\n",
            "'zens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but th'\n",
            "105\n",
            "'e superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we a'\n",
            "105\n",
            "'re too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particula'\n",
            "105\n",
            "'rise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we bec'\n",
            "105\n",
            "'ome rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSecond Cit'\n",
            "106\n",
            "\"izen:\\nWould you proceed especially against Caius Marcius?\\n\\nAll:\\nAgainst him first: he's a very dog to\"\n",
            "107\n",
            "' the commonalty.\\n\\nSecond Citizen:\\nConsider you what services he has done for his country?\\n\\nFirst Citi'\n",
            "108\n",
            "'zen:\\nVery well; and could be content to give him good\\nreport fort, but that he pays himself with bein'\n",
            "105\n",
            "'g proud.\\n\\nSecond Citizen:\\nNay, but speak not maliciously.\\n\\nFirst Citizen:\\nI say unto you, what he hat'\n",
            "109\n",
            "'h done famously, he did\\nit to that end: though soft-conscienced men can be\\ncontent to say it was for '\n",
            "105\n",
            "'his country he did it to\\nplease his mother and to be partly proud; which he\\nis, even till the altitud'\n",
            "105\n",
            "'e of his virtue.\\n\\nSecond Citizen:\\nWhat he cannot help in his nature, you account a\\nvice in him. You m'\n",
            "107\n",
            "'ust in no way say he is covetous.\\n\\nFirst Citizen:\\nIf I must not, I need not be barren of accusations;'\n",
            "106\n",
            "\"\\nhe hath faults, with surplus, to tire in repetition.\\nWhat shouts are these? The other side o' the ci\"\n",
            "105\n",
            "'ty\\nis risen: why stay we prating here? to the Capitol!\\n\\nAll:\\nCome, come.\\n\\nFirst Citizen:\\nSoft! who co'\n",
            "110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First Citizen >> irst Citizen"
      ],
      "metadata": {
        "id": "3dhSbB-4xaPc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]   # First Ctizen(not including the space at the end)\n",
        "  target_text = chunk[1:]   # irst Citizen\n",
        "  return input_text,target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n"
      ],
      "metadata": {
        "id": "zHvZ40d7xzdE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"input_data\\n\")\n",
        "  print(repr(\"\".join([idx2char[i] for i in input_example.numpy()])))\n",
        "  print(\"\\ntarget_data\\n\")\n",
        "  print(repr(\"\".join([idx2char[i] for i in target_example.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxqHs4XFyb8A",
        "outputId": "c9fc2124-dcf6-4f68-cb35-ee026dc56425"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_data\n",
            "\n",
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "\n",
            "target_data\n",
            "\n",
            "'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(Config.buffer_size).batch(Config.batch_size, drop_remainder =True) # Shuffling the dataset and creating batches\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfHivgF32B7p",
        "outputId": "976cc3f8-b24c-48ee-d48c-62bd862fbf0f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Config.vocab_size = len(characters)\n",
        "Config.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6f3lpEp3kAC",
        "outputId": "13c0ec29-e912-4ab5-f8d3-d2b46f7b3fa4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape = [batch_size, None]),\n",
        "                               tf.keras.layers.GRU(rnn_units, return_sequences = True, stateful =  True, recurrent_initializer = 'glorot_uniform'),\n",
        "                               tf.keras.layers.Dense(vocab_size)\n",
        "                               ])\n",
        "  return model\n",
        "\n",
        "# return sequences is kept true because it helps in output of each rnn unit\n",
        "# stateful = True (means from one epoch to another epoch the hidden state is passed on, remembers previous state)\n",
        "# hidden state means the feedback taht we had in case of rnns in each loop"
      ],
      "metadata": {
        "id": "I2NtfK1w3701"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model (vocab_size =Config.vocab_size,\n",
        "                    embedding_dim = Config.embedding_dim,\n",
        "                    rnn_units = Config.rnn_units,\n",
        "                    batch_size = Config.batch_size)"
      ],
      "metadata": {
        "id": "DLtggznLAo-e"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr6I6eoPCD3S",
        "outputId": "1c9fe29d-2a07-4946-ecc5-9220d32f6eb7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " gru (GRU)                   (64, None, 1024)          3938304   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,logits, from_logits = True)\n",
        "\n",
        "#logits here means the absence of activation function"
      ],
      "metadata": {
        "id": "-rsfpO_gCdPI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam', loss = loss)"
      ],
      "metadata": {
        "id": "CMNPxsrDF3ds"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_prefix = os.path.join(Config.checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix,\n",
        "    save_weights_only =True\n",
        ")"
      ],
      "metadata": {
        "id": "fy8stX2dGV1p"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history =  model.fit(dataset, epochs = Config.epochs, callbacks = [checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SqoS6aYGWVI",
        "outputId": "d14c8260-814f-456c-9897-795e1b5b80c8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "172/172 [==============================] - 17s 55ms/step - loss: 2.6666\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.9591\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.6941\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.5464\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.4574\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.3971\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.3512\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.3124\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2778\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.2446\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2127\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.1808\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.1473\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.1153\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.0791\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.0433\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.0070\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.9722\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.9356\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.9028\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.8714\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.8400\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.8140\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7901\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7684\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7512\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7342\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.7208\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7080\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.6982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how the 172 (steps per epoch) came in model training? character dataset divided into 101 which was divided furhter into batches\n",
        "(len(text)/ Config.batch_size)/(Config.seq_length + 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrnUHzkNGXDE",
        "outputId": "0716481a-8d40-4387-f955-d00234d6bfc7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "172.55476485148515"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#restoring checkpoint\n",
        "tf.train.latest_checkpoint(Config.checkpoint_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2msBshvpK0JO",
        "outputId": "9d6fa51c-1543-4e58-cf23-0a756e4accdf"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_ckpt/ckpt_30'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_from_ckpt =  build_model (vocab_size =Config.vocab_size,\n",
        "                    embedding_dim = Config.embedding_dim,\n",
        "                    rnn_units = Config.rnn_units,\n",
        "                    batch_size = 1)\n",
        "\n",
        "# when we were creating the checkpoints we were only saving the weights we were not saving the architecture of thr model\n",
        "# so we again create the same model with same architecture where we load the weights which are coming from the latest checkpoint "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiKgN0dTOMsM",
        "outputId": "31188870-50eb-4746-fb94-56801643a04d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_from_ckpt.load_weights(tf.train.latest_checkpoint(Config.checkpoint_dir))\n",
        "\n",
        "model_from_ckpt.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "s7Xxk5W3Q_Zj"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_from_ckpt.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuVlDyOXQRP0",
        "outputId": "3911081f-8dae-4499-fe4b-305492a3485e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (1, None, 256)            16640     \n",
            "                                                                 \n",
            " gru_3 (GRU)                 (1, None, 1024)           3938304   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (1, None, 65)             66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREDICTION PROCEDURE"
      ],
      "metadata": {
        "id": "z6osMohUSRre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function\n",
        "def generate_text(model, start_string, no_of_chars_to_gen = 1000):\n",
        "   # convert input text to numbers\n",
        "   input_val = [char_dict[s] for s in start_string]\n",
        "   input_val = tf.expand_dims(input_val, 0) # [] >> [[]]\n",
        "\n",
        "   text_generated = list()\n",
        "   temperature = 1.0\n",
        "\n",
        "   model.reset_states()  # resetting the previous states if any while predictions\n",
        "   for i in range(no_of_chars_to_gen):\n",
        "     predictions = model(input_val)\n",
        "\n",
        "     predictions = tf.squeeze(predictions, 0)\n",
        "     predicted_id = tf.random.categorical(predictions, num_samples =1)[-1, 0].numpy()\n",
        "\n",
        "     input_val = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "     text_generated.append(idx_2_char[predicted_id])\n",
        "\n",
        "   return start_string + \"\".join(text_generated)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J8RVHGWeQU2r"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model, start sequence and the no: of characters to generaate is taken in as arguments of the above function\n",
        "# model we have takes one input  and gives the probability outcome from among the 65 with highest value\n",
        "# use of temperature is that it diminishes the differences of the numerical form of characters for random.categorical to distinguish\n",
        "# instead of using softmax for output tf.random.categorical was used\n",
        "# softmax gives hard predictions whereas tf.random.categorical will look at others nearby probabilities as well"
      ],
      "metadata": {
        "id": "K9UaQm4GfHQ9"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F5ATSg1siwT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result =generate_text(model = model_from_ckpt, start_string= \"Romeo\", no_of_chars_to_gen =1000)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df1Oaxqsik-V",
        "outputId": "b764dce9-e67f-4e18-bbfc-53933018d4e5"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Romeome, while we must comblin and throw a fit;\n",
            "He shall you feel the Duke of Hereford, away rately must was?\n",
            "How shall them now?\n",
            "\n",
            "SICINIUS:\n",
            "For stronger than the interruption dried it,\n",
            "Warwick ne'er so little in ruch all:\n",
            "My lord Shapot of compassion with we it in my ugreems\n",
            "bestir: but now, might have is dail!\n",
            "\n",
            "GRUMIO:\n",
            "O my dear, be gone, master right.\n",
            "How will he will stand for conscience, hath a son of York;\n",
            "And neither chafed my hurdies and whiteness;\n",
            "You may last royal father.'\n",
            "\n",
            "Second Lord:\n",
            "For death, and we'll no longer stay.\n",
            "\n",
            "SEBASTIAN:\n",
            "One be a chminstre for.\n",
            "\n",
            "JULIET:\n",
            "Ay, madam, you have braving him o'er a courtier's nost,\n",
            "And, whilst I descend on thee.\n",
            "But since comes here; why, so! do you speak?\n",
            "But ne'er shall come to rage my life of death.\n",
            "Here do we make feed your lordship's man.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Think you, main, doubt not of him?\n",
            "\n",
            "POLIXENES:\n",
            "When will I luneway.\n",
            "\n",
            "Margar:\n",
            "Men grace with you and yours, and royalty he chysilf my life as it\n",
            "will answer that the quarrel from his head\n"
          ]
        }
      ]
    }
  ]
}