{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAKAAASSHHH24/NLP-BASICS/blob/main/Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqjfGZoPyuLS"
      },
      "source": [
        "# Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9QEQiVQyuLX"
      },
      "source": [
        "# **One-Hot encoding**\n",
        "\n",
        "**Why is it called one-hot?** \n",
        "\n",
        "**After each word is one-hot encoded, only one position has an element of 1 and the other positions are all 0.**\n",
        "\n",
        "For example, \n",
        "the sentence **\"the boy is crying\"** (assuming there are only four English words in the world), after one-hot encoding,\n",
        "\n",
        "**the corresponds to (1, 0, 0, 0)**\n",
        "\n",
        "**boy corresponds to (0, 1, 0 ， 0）**\n",
        "\n",
        "**is corresponds to (0,0,1,0)**\n",
        "\n",
        "**crying corresponds to (0,0,0,1)**\n",
        "\n",
        "Each word corresponds to a position in the vector, and this position represents the word.\n",
        "\n",
        "But this way requires a very high dimension, because if all vocabularies have 100,000 words, then each word needs to be represented by a vector of length 100,000.\n",
        "\n",
        "**the corresponding to (1, 0, 0, 0, ..., 0) (length is 100,000)**\n",
        "\n",
        "**boy corresponding to (0, 1, 0, 0, ..., 0)**\n",
        "\n",
        "**is corresponding to (0, 0, 1, 0 , ..., 0)**\n",
        "\n",
        "**crying corresponds to (0,0,0,1, ..., 0) to get high-dimensional sparse tensors.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4K2mc5nyuLY"
      },
      "source": [
        "### Disadvatages of One HotEncoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBQJFYbByuLZ"
      },
      "source": [
        "One-Hot coding is simple and easy to use, the disadvantages are also obvious:\n",
        "\n",
        ">The length of the word vector is equal to the length of the vocabulary, and the word vector is extremely sparse. When the vocabulary is large, the computational complexity will be very large.\n",
        "\n",
        ">Any two words are orthogonal, meaning that the relationship between words cannot be obtained from the One-Hot code\n",
        "\n",
        ">The distance between any two words is equal, and the semantic relevance of the two words cannot be reflected from the distance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hiSfIqPyuLa"
      },
      "source": [
        "## **Embedding**\n",
        "In contrast, word embedding embeds words into a low-dimensional dense space.\n",
        "\n",
        "For example, the same **\"the boy is crying\"** sentence (assuming that there are only 4 English words in the world), after encoding, it may become:\n",
        "\n",
        "**the corresponding (0.1)**\n",
        "\n",
        "**boy corresponding (0.14)**\n",
        "\n",
        "**is corresponding (0)**\n",
        "\n",
        "**crying corresponding (0.82)**\n",
        "\n",
        "We assume that the embedded space is 256 dimensions (generally 256, 512 or 1024 dimensions, the larger the vocabulary, the higher the corresponding spatial dimension)\n",
        "\n",
        "**Then\n",
        "the corresponding (\n",
        "0.1,\n",
        "0.2, 0.4,\n",
        "0 , ...) (vector length is 256) boy corresponds to (0.23, 0.14, 0, 0 , ...) is corresponding to (0, 0 , 0.41, 0.9, ...) , 0.82, 0, 0.14, ...)**\n",
        "\n",
        "One-hot encoding is very simple, but the spatial dimension is high and for one-hot encoding, the distance between any two words is $$\\sqrt{2}$$.\n",
        "\n",
        " But in practice, the word **(boy) to word (man) should be very close** (because they are closely related), and **the word (cat) to word (stone) should be very far** (because they are basically unrelated).\n",
        "\n",
        "Embedding space has low dimensions and allows space to have structure .\n",
        "\n",
        "For example, the distance between the vectors can reflect gender, age, etc. (this requires training, and the unembedding layer has no structure), for example:\n",
        "\n",
        "**man-woman = boy-girl**\n",
        "\n",
        "**man-daddy = woman-mother**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bfuJv-9yuLa"
      },
      "source": [
        "In Keras, the Embedding layer requires two parameters, one is the number of words in the token, and the other is the embedded dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IF5F9jyCyuLb"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(1000,64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SEy1dnWyuLh"
      },
      "source": [
        "1000 : The length of the token is 1000 (can be considered as the number of all words in the vocabulary)\n",
        "\n",
        "64 : Represents embedded 64-dimensional space (64 attributes can be considered, such as imaginary adult eye shape, nose shape, mouth shape, height, weight, age, etc., together, it is a person (word). A word, a thousand such words means all the words in the vocabulary)\n",
        "\n",
        "Embedding layer input : a two-dimensional tensor with the shape (samples, sequential_length)\n",
        "samples: represent different sentences.\n",
        "sequential_length: represents the number of words in the sentence, each word corresponds to a number, a total of sequential_length words.\n",
        "\n",
        "The output of the embedding layer : a three-dimensional tensor with the shape (samples, sequential_length, dimensionality)\n",
        "samples: Represent different sentences.\n",
        "sequential_length: represents the number of words in a sentence.\n",
        "dimensionality: represents the number of channels. A vector of values ​​on all channels on the same samples and the same sequential_length represents a word, such as (0,0, :) represents a word.\n",
        "\n",
        "The embedding layer can be regarded as a matrix , assuming that the input is (100, 20), 100 sequences of length 20, the vocabulary length is 10000, Embedding (10000, 8), and the output is (100, 20, 8)\n",
        "because After one-hot encoding, each sequence can be regarded as (20, 10000), and the matrix of (10000, 8) is multiplied to get the matrix of (20, 8), so 100 such sequences pass through the embedding layer. Becomes (100, 20, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ptRi7XJiyuLi",
        "outputId": "a64315f1-db47-4134-d825-3f5e0134ad23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 20, 8)             80000     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 160)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 161       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Instantiate an Embedding layer\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten,Dense,Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000,8,input_length=20))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dH7eqsf_yuLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936a46f4-97fa-41bf-9c29-cac9bee41085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten,Dense,Embedding\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 20\n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=10000)\n",
        "#(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "x_train = pad_sequences(x_train,maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test,maxlen=maxlen)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(10000,8,input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1,activation='sigmoid'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "--uzhSSayuLp",
        "outputId": "cd77861d-4b81-484f-8611-ab556c7118dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 2s 3ms/step - loss: 0.6735 - acc: 0.6136 - val_loss: 0.6275 - val_acc: 0.6944\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
        "history = model.fit(x_train,y_train,epochs=1,batch_size=32,validation_split=0.2,verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoHp5o9TyuLt"
      },
      "source": [
        "#### It can be seen that in the embedding layer, we need to train 8 times 10000 = 80,000 parameters. Each row in the trained embedding layer represents a vector of words.\n",
        "\n",
        ">Use the Embedding layer and classifier on IMDB data.\n",
        "\n",
        ">The imdb data set built in keras has classified positive and negative evaluations and vectorized evaluation content. We use the Embedding layer and classifier to train a neural network. How well did it perform.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk7-H5FpyuLu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqyQ54_kyuL0"
      },
      "source": [
        "#### Draw a picture and observe a wave carefully:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lk_C-IpVyuL1",
        "outputId": "f638c483-1b15-4915-d8bd-0a7b7d0c8f59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc=history.history['acc']\n",
        "val_acc=history.history['val_acc']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(1,len(acc)+1)\n",
        "\n",
        "plt.plot(epochs,acc,'bo',label='acc')\n",
        "plt.plot(epochs,val_acc,'b',label='val_acc')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs,loss,'bo',label='loss')\n",
        "plt.plot(epochs,val_loss,'b',label='val_loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_9w0BPMyuL4"
      },
      "source": [
        "#### It can be seen that the verification accuracy is about 75%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf3CNxioyuL4"
      },
      "source": [
        "# BOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P53EmPynyuL5"
      },
      "source": [
        "**Bag-of-words model is a commonly used document representation method in the field of information retrieval .**\n",
        "\n",
        ">In information retrieval, the BOW model assumes that for a document, it ignores its word order, grammar, syntax and other factors, and treats it as a collection of several words. The appearance of each word in the document is independent and independent of whether other words appear. **(It's out of order)**\n",
        "\n",
        ">The Bag-of-words model (BoW model) ignores the grammar and word order of a text, and uses a set of unordered words to express a text or a document.\n",
        "\n",
        "#### Let's take an example\n",
        "\n",
        "`John likes to watch movies. Mary likes too.`\n",
        "\n",
        "`John also likes to watch football games.`\n",
        "\n",
        "Build a dictionary based on the words that appear in the above two sentences:\n",
        "\n",
        "`{\"John\": 1, \"likes\": 2, \"to\": 3, \"watch\": 4, \"movies\": 5, \"also\": 6, \"football\": 7, \"games\": 8, \"Mary\": 9, \"too\": 10}`\n",
        "\n",
        "\n",
        "The dictionary contains 10 words, each word has a unique index. Note that their order is not related to the order in which they appear in the sentence. According to this dictionary, we re-express the above two sentences into the following two vectors:\n",
        "\n",
        "`[1, 2, 1, 1, 1, 0, 0, 0, 1, 1]`\n",
        "\n",
        "`[1, 1, 1, 1, 0, 1, 1, 1, 0, 0]`\n",
        "\n",
        "\n",
        "These two vectors contain a total of 10 elements, where the i-th element represents the number of times the i-th word in the dictionary appears in the sentence. \n",
        "\n",
        "Now imagine a **huge document set D with a total of M documents**. After all the words in the document are extracted, they form a dictionary containing N words. Using the Bag-of-words model, **each document can be represented as an N-dimensional vector**.\n",
        "\n",
        "\n",
        "Therefore, the BoW model can be considered as a statistical histogram. It is used in text retrieval and processing applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU0XAU4qyuL5"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk0Eg4qdyuL6"
      },
      "source": [
        "**TF-IDF (Term Frequency-Inverse Document Frequency)**, a commonly used weighting technique for information retrieval and information exploration.\n",
        "\n",
        "TF-IDF is a statistical method used to evaluate the importance of a word to a file set or a file in a corpus. The importance of the word increases in proportion to the number of times it appears in the file, but at the same time decreases inversely with the frequency of its appearance in the corpus.\n",
        "\n",
        "* **Term frequency TF (item frequency)**: number of times a given word appears in the text. This number is usually normalized (the numerator is generally smaller than the denominator) to prevent it from favoring long documents, because whether the term is important or not, it is likely to appear more often in long documents than in paragraph documents.\n",
        "\n",
        "> **TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).**\n",
        "\n",
        "Term frequency (TF) indicates how often a term (keyword) appears in the text .\n",
        "\n",
        "This number is usually normalized (usually the word frequency divided by the total number of words in the article) to prevent it from favoring long documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIJIuSpMyuL7"
      },
      "source": [
        "**Formula of Tf**    ![title](img/tf.png)\n",
        "\n",
        "\n",
        "where  ni, j  is the number of occurrences of the word in the file  dj  , and the denominator is the sum of the occurrences of all words in the file dj;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVP7FEGoyuL7"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJzZpE-JyuL8"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Dwu6mPKyuL8"
      },
      "source": [
        "* **Inverse document frequency (IDF)**: A measure of the general importance of a word. The main idea is that if there are fewer documents containing the entry t and the larger, it means that the entry has a good ability to distinguish categories. The IDF of a specific word can be calculated by dividing the total number of files by the number of files containing the word, and then taking the log of the obtained quotient.\n",
        "\n",
        ">**IDF(t) = log_e(Total number of documents / Number of documents with term t in it).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T26bTkv0yuL9"
      },
      "source": [
        "**Formula of Idf**  ![title](img/idf1.png)\n",
        "\n",
        "\n",
        "among them\n",
        "\n",
        "* | D |: Total number of files in the corpus\n",
        "\n",
        "* |  {  $j: $t_{i}$ \\in $d_{j} $$  }  | : The number of files containing words $t_{i}$ ( $n_{i,j}$ $\\neq$ 0 , the number of files). If the word is not in the corpus, it will cause the dividend to be zero, so it is generally used.1 + |  {  $j :$t_{i}$ \\in $d_{j}$$  }  |.\n",
        "\n",
        "\n",
        "**So, Formula of tf-Idf** ![title](img/ttttt.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n2YUwi1yuL-"
      },
      "source": [
        "#### Example:\n",
        "\n",
        "Consider a document containing 100 words where in the word cat appears 3 times. \n",
        "\n",
        "The **term frequency (Tf) for cat** is then **(3 / 100) = 0.03**. Now, assume we have 10 million documents and the word cat appears in one thousand of these.\n",
        "\n",
        "Then, the **inverse document frequency (Idf)** is calculated as **log(10,000,000 / 1,000) = 4.** \n",
        "\n",
        "Thus, the **Tf-idf** weight is the product of these quantities: **0.03 * 4 = 0.12.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGd8-mdVyuMA"
      },
      "source": [
        "#### TF-IDF application\n",
        "\n",
        "1.  **Search engine**\n",
        "2.  **Keyword extraction**\n",
        "3.  **Text similarity**\n",
        "4.  **Text summary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AScJZdWYyuMB"
      },
      "source": [
        "## Code Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3sOgkIoyuMB"
      },
      "source": [
        "### Python3 implements TF-IDF algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stTIGcpayuMC",
        "outputId": "2069fa61-16dd-4bc2-d92e-93cb6f869c65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('to', 0.0322394037469742), ('stop', 0.0322394037469742), ('worthless', 0.0322394037469742), ('my', 0.028288263356383563), ('dog', 0.028288263356383563), ('him', 0.028288263356383563), ('stupid', 0.028288263356383563), ('has', 0.025549122992281622), ('flea', 0.025549122992281622), ('problems', 0.025549122992281622), ('help', 0.025549122992281622), ('please', 0.025549122992281622), ('maybe', 0.025549122992281622), ('not', 0.025549122992281622), ('take', 0.025549122992281622), ('park', 0.025549122992281622), ('dalmation', 0.025549122992281622), ('is', 0.025549122992281622), ('so', 0.025549122992281622), ('cute', 0.025549122992281622), ('I', 0.025549122992281622), ('love', 0.025549122992281622), ('posting', 0.025549122992281622), ('garbage', 0.025549122992281622), ('mr', 0.025549122992281622), ('licks', 0.025549122992281622), ('ate', 0.025549122992281622), ('steak', 0.025549122992281622), ('how', 0.025549122992281622), ('quit', 0.025549122992281622), ('buying', 0.025549122992281622), ('food', 0.025549122992281622)]\n",
            "32\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import operator\n",
        " \n",
        "\n",
        "def loadDataSet():\n",
        "    dataset = [ ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],    \n",
        "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
        "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
        "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
        "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
        "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid'] ]\n",
        "    classVec = [0, 1, 0, 1, 0, 1]  \n",
        "    return dataset, classVec\n",
        " \n",
        "\n",
        "def feature_select(list_words):\n",
        "    \n",
        "    doc_frequency=defaultdict(int)\n",
        "    for word_list in list_words:\n",
        "        for i in word_list:\n",
        "            doc_frequency[i]+=1\n",
        " \n",
        "    \n",
        "    word_tf={}  \n",
        "    for i in doc_frequency:\n",
        "        word_tf[i]=doc_frequency[i]/sum(doc_frequency.values())\n",
        " \n",
        "    \n",
        "    doc_num=len(list_words)\n",
        "    word_idf={} \n",
        "    word_doc=defaultdict(int) \n",
        "    for i in doc_frequency:\n",
        "        for j in list_words:\n",
        "            if i in j:\n",
        "                word_doc[i]+=1\n",
        "    for i in doc_frequency:\n",
        "        word_idf[i]=math.log(doc_num/(word_doc[i]+1))\n",
        " \n",
        "    \n",
        "    word_tf_idf={}\n",
        "    for i in doc_frequency:\n",
        "        word_tf_idf[i]=word_tf[i]*word_idf[i]\n",
        " \n",
        "    \n",
        "    dict_feature_select=sorted(word_tf_idf.items(),key=operator.itemgetter(1),reverse=True)\n",
        "    return dict_feature_select\n",
        " \n",
        "if __name__=='__main__':\n",
        "    data_list,label_list=loadDataSet() \n",
        "    features=feature_select(data_list) \n",
        "    print(features)\n",
        "    print(len(features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB-5ItW_yuMF",
        "outputId": "42909384-1ae5-406a-a2fc-9fd6dd8c92c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Using cached https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip\n",
            "Requirement already satisfied: six in c:\\users\\soura\\anaconda3\\envs\\tensor\\lib\\site-packages (from nltk) (1.12.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py): started\n",
            "  Building wheel for nltk (setup.py): finished with status 'done'\n",
            "  Stored in directory: C:\\Users\\soura\\AppData\\Local\\pip\\Cache\\wheels\\96\\86\\f6\\68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "Successfully installed nltk-3.4.5\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXNYTNgYyuMI"
      },
      "source": [
        "### NLTK implements the TF-IDF algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCjMMcmWyuMI",
        "outputId": "827eb657-f83a-4628-83c6-d7f022bec034"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['this', 'is', 'sentence', 'one'], ['this', 'is', 'sentence', 'two'], ['this', 'is', 'sentence', 'three']]\n",
            "<Text: this is sentence one this is sentence two...>\n",
            "0.08333333333333333\n",
            "1.0986122886681098\n",
            "0.0915510240556758\n"
          ]
        }
      ],
      "source": [
        "from nltk.text import TextCollection\n",
        "from nltk.tokenize import word_tokenize\n",
        " \n",
        "sents=['this is sentence one','this is sentence two','this is sentence three']\n",
        "sents=[word_tokenize(sent) for sent in sents]\n",
        "print(sents)\n",
        "corpus=TextCollection(sents)\n",
        "print(corpus)\n",
        " \n",
        "tf=corpus.tf('one',corpus)\n",
        "print(tf)\n",
        " \n",
        "idf=corpus.idf('one')\n",
        "print(idf)\n",
        " \n",
        "tf_idf=corpus.tf_idf('one',corpus)\n",
        "print(tf_idf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9n624C3yuML"
      },
      "source": [
        "### Sklearn implements TF-IDF algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GfNfRBSyuMM",
        "outputId": "92169ae9-418e-45d1-b05a-99cc1266b71b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output x_train text vector：\n",
            "[[0.22941573 0.22941573 0.22941573 0.45883147 0.22941573 0.22941573\n",
            "  0.22941573 0.22941573 0.45883147 0.45883147]]\n",
            "Output x_test text vector：\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        " \n",
        "x_train = ['The main idea of TF-IDF is that algorithm is an important feature that can be separated from the corpus background']\n",
        "x_test=['Original text marked ',' main idea']\n",
        " \n",
        "vectorizer = CountVectorizer(max_features=10)\n",
        "\n",
        "tf_idf_transformer = TfidfTransformer()\n",
        "\n",
        "tf_idf = tf_idf_transformer.fit_transform(vectorizer.fit_transform(x_train))\n",
        "\n",
        "x_train_weight = tf_idf.toarray()\n",
        " \n",
        "\n",
        "tf_idf = tf_idf_transformer.transform(vectorizer.transform(x_test))\n",
        "x_test_weight = tf_idf.toarray()\n",
        " \n",
        "print('Output x_train text vector：')\n",
        "print(x_train_weight)\n",
        "print('Output x_test text vector：')\n",
        "print(x_test_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQe105WeyuMO"
      },
      "outputs": [],
      "source": [
        "Keywords are words that can express the content of the center of a document.\n",
        "Information retrieval and system collection for reader review. Keyword extraction is a branch of the field of text mining.\n",
        "Basic work of text mining research such as document comparison, abstract generation, document classification and clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOCWnV-xyuMT"
      },
      "source": [
        "### Jieba implements TF-IDF algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfac_ExTyuMU",
        "outputId": "2c8bed82-219f-4bd2-ce4e-b0341e66b998"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache C:\\Users\\soura\\AppData\\Local\\Temp\\jieba.cache\n",
            "Loading model cost 1.322 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['document', 'text', 'mining', 'Keywords', 'words']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import jieba.analyse\n",
        " \n",
        "text='Keywords are words that can express the content of the center of a document.Information retrieval and system collection for reader review. Keyword extraction is a branch of the field of text mining.Basic work of text mining research such as document comparison, abstract generation, document classification and clustering'\n",
        " \n",
        "keywords=jieba.analyse.extract_tags(text, topK=5, withWeight=False, allowPOS=())\n",
        "print(keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnibpAlbyuMX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39x0oBreyuMa"
      },
      "source": [
        "# n GRAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIKOVLUfyuMa"
      },
      "source": [
        "**Wikipedia definition**: \n",
        "\n",
        "**In computational linguistics, n-gram refers to n consecutive items in the text (items can be phoneme, syllable, letter, word or base pairs)**\n",
        "\n",
        "N-grams of texts are widely used in the field of text mining and natural language processing. They are basically a set of co-occurring words within a defined window and when computing the n-grams, we typically move one word forward or more depending upon the scenario.\n",
        "\n",
        ">For example, for the sentence **“The cow jumps over the moon”**. If **N=2** (known as bigrams), then the ngrams would be:\n",
        "\n",
        "* the cow\n",
        "* cow jumps\n",
        "* jumps over\n",
        "* over the\n",
        "* the moon\n",
        "\n",
        "In n-gram, **n = 1 is unigram**, **n = 2 is bigram**, **n = 3 is trigram**. \n",
        "\n",
        "After **n> 4**, refer directly to numbers, such as **4-gram, 5-gram**.\n",
        "\n",
        "gram is often used to compare sentence similarity, fuzzy query, sentence rationality, sentence correction, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xs2QJmjyuMa"
      },
      "source": [
        "The n-gram can represent the semantic association reflected by the positional relationship between words. Before explaining the n-gram, we derive from the initial sentence probability.\n",
        "\n",
        "Suppose a sentence S is an ordered arrangement of n words, and is written as: ![title](img/gif.gif)\n",
        "\n",
        "\n",
        "We will abbreviate it as W_ {1} ^ {n}, then the probability of this sentence is: ![title](img/ng.gif)\n",
        "\n",
        "For a single probability, which means the probability that the word appears in the case given by the previous word, we can use Bayesian formula to get: ![title](img/ng1.gif)\n",
        "\n",
        "The last item is the frequency in the corpus. However, long sentences or text after depunctuation may be very long, and the words that are too early have a small impact on the prediction of the word, so we use Markov's hypothesis that the probability of taking the word depends only on the front of the word the n-1 words, this is the idea n-gram model.\n",
        "\n",
        "So the above formula becomes: ![title](img/ng2.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdPjrowYyuMb"
      },
      "source": [
        "#### Determination of N in N-gram\n",
        "\n",
        "To confirm the value of N. \"Language Modeling with Ngrams\" uses the indicator **Perplexity**. The smaller the indicator, the better the effect of a language model. \n",
        "\n",
        ">The article uses a Wall Street Journal database with a dictionary size of 19,979. The training set contains 38 million words and the test set contains 1.5 million words. \n",
        "\n",
        "For different N-grams, calculate their respective purplexity.\n",
        "\n",
        "![title](img/formula.png)\n",
        "\n",
        "The results show that Tri-gram's Perplexity is the smallest, so it works best.\n",
        "![title](img/result.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8dpWgzryuMb"
      },
      "source": [
        "### Unigram Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YLq3kK2yuMc",
        "outputId": "ecfa1476-a33b-415f-94ca-a965f01e71e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', ' ', 'am', ' ', 'going', ' ', 'to', ' ', 'the', ' ', 'United', ' ', 'States']\n"
          ]
        }
      ],
      "source": [
        "import jieba\n",
        " \n",
        "text = \"I am going to the United States\"\n",
        "cut = jieba.cut(text)\n",
        "sent = list(cut)\n",
        "print(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-ye2OG2yuMe"
      },
      "source": [
        "### Bigram Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pktQcu1tyuMf",
        "outputId": "7f7072f7-d0a2-4ed3-c608-9adcc9d7974c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I will', 'will go', 'go to', 'to United', 'United States']\n"
          ]
        }
      ],
      "source": [
        "Sent = \"I will go to United States\"\n",
        "lst_sent = Sent.split (\" \")\n",
        "of_bigrams_in = []\n",
        "for i in range(len(lst_sent)- 1):\n",
        "   of_bigrams_in.append(lst_sent[i]+ \" \" + lst_sent[ i + 1])\n",
        "   \n",
        "    \n",
        "print(of_bigrams_in)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFklgTn9yuMi"
      },
      "source": [
        "### Trigram Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo-_ANz0yuMj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "punctuation_pattern = re.compile(r\"\" \"[.,!? \"\"] \"\" \" )\n",
        "\n",
        "sent = \"I will go to United States\"\n",
        "no_punctuation_sent = re.sub(punctuation_pattern , \" \" , sent )\n",
        "lst_sent = no_punctuation_sent.split (\" \")\n",
        "trigram = []\n",
        "for i in range(len(lst_sent)- 2):\n",
        "   trigram.append(lst_sent[i] + \" \" + lst_sent[i + 1] + \" \" +lst_sent[i + 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKrjCaxTyuMl",
        "outputId": "3d7ac081-a5e8-42b6-a92d-8d487c6a0128"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I will go', 'will go to', 'go to United', 'to United States']"
            ]
          },
          "execution_count": 35,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trigram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvpDZKy1yuMq"
      },
      "source": [
        "## Co-occurrence matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu6-40ZJyuMq"
      },
      "source": [
        ">The co-occurrence matrix is ​​also expressed by considering the relationship between words in the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EISAu7YcyuMr"
      },
      "source": [
        ">A very important idea is that we think that the meaning of a word is closely related to the word next to it. This is where we can set a window (the size is generally 5 ~ 10). The size of the window below is 2, so in this window, the words that appear with rests are life, he, in, and peace. Then we use this co-occurrence relationship to generate word vectors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvezPK9PyuMs"
      },
      "source": [
        "![title](img/concurrence.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YarWzEPyuMs"
      },
      "source": [
        "For example, our corpus now includes the following three documents:\n",
        "\n",
        "#### I like deep learning.\n",
        "\n",
        "#### I like NLP.\n",
        "\n",
        "#### I enjoy flying.\n",
        "\n",
        "As an example, **we set the window size to 1**, which means that **we only look at the word immediately surrounding a word**. At this point, you will get a symmetric matrix-co-occurrence matrix. Because in our corpus, **the number of times I and like appear as neighbors in the window at the same time is 2**, the value where I and like intersect in the table below is 2. \n",
        "\n",
        "In this way, the idea of turning words into vectors is done. Each row (or each column) of the co-occurrence matrix is a vector representation of the corresponding word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8RAM0SjyuMs"
      },
      "source": [
        "![title](img/concur.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2ke8lX0yuMt"
      },
      "source": [
        ">Although the Cocurrence matrix solves the relative position between words to some extent, this problem should be paid attention to. But it still faces dimensional disaster. \n",
        "\n",
        ">In other words, the vector representation of a word is too long. At this time, it is natural to think of some common dimensionality reduction methods such as SVD or PCA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DReSxvyOyuMt"
      },
      "source": [
        ">The selection of the window size is the same as determining n in the n-gram. The size of the matrix will also increase when the window is enlarged, so it still has a large amount of calculation in nature, and the SVD algorithm has a large amount of calculation. If the text set is very More, it is not operable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Rtzkmq9yuMu"
      },
      "source": [
        "# GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mw2xbiuyuMu"
      },
      "source": [
        "**GloVe is an unsupervised learning algorithm for obtaining vocabulary vector representations. The aggregated global word co-occurrence statistics from the corpus are trained and the resulting representations show interesting linear substructures of the word vector space.**\n",
        "\n",
        "\n",
        "Official website homepage address: <a href=\"https://nlp.stanford.edu/projects/glove/\" target=\"_blank\">https://nlp.stanford.edu/projects/glove/</a>\n",
        "\n",
        "Github: <a href=\"https://github.com/stanfordnlp/GloVe\" target=\"_blank\">https://github.com/stanfordnlp/GloVe</a>\n",
        "\n",
        "Paper download address: <a href=\"https://nlp.stanford.edu/pubs/glove.pdf\" target=\"_blank\">https://nlp.stanford.edu/pubs/glove.pdf</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbAVxWGHyuMx"
      },
      "source": [
        "#### GloVe word vector format\n",
        "\n",
        "GloVe is a type of Word embedding. The format of the GloVe word vector and word2vec is a little different from the Stanford open source code training. **The first line of the model trained by word2vec is: thesaurus size and dimensions, while gloVe does not**\n",
        "\n",
        "Word2vec training format:\n",
        "\n",
        "    Size Dimension\n",
        "\n",
        "    Word1 vector1\n",
        "    Word2 vector1\n",
        "    ....\n",
        "    WordN vectorN\n",
        "    \n",
        "\n",
        "GloVe training format:\n",
        "\n",
        "\n",
        "    Word1 vector1\n",
        "    Word2 vector1\n",
        "    ....\n",
        "    WordN vectorN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th6C8UCsyuMy"
      },
      "source": [
        ">Therefore, we use the model trained by Glove to add a line of Vocabulary Size in front, and the model is used in the same way as word2vec. The official website provides a lot of word vector models trained using thesaurus, which can be downloaded and used directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru8DqzT0yuMz"
      },
      "source": [
        "![title](img/glove.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Glove Practical\n",
        "\n",
        "Download link: https://www.kaggle.com/datasets/danielwillgeorge/glove6b100dtxt"
      ],
      "metadata": {
        "id": "xxi2Vzo2Rekr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "df = pd.read_csv('data/Twitter Sentiments.csv')\n",
        "# drop the columns\n",
        "df = df.drop(columns=['id', 'label'], axis=1)\n",
        "\n",
        "df['clean_text'] = df['tweet'].str.lower()\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in text.split() if word not in STOPWORDS])\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: remove_stopwords(x))\n",
        "\n",
        "import re\n",
        "def remove_spl_chars(text):\n",
        "    text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: remove_spl_chars(x))\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "G-FmZ8-nRiMZ",
        "outputId": "9972ed16-673c-4a6e-e4da-77c65fff5c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               tweet  \\\n",
              "0   @user when a father is dysfunctional and is s...   \n",
              "1  @user @user thanks for #lyft credit i can't us...   \n",
              "2                                bihday your majesty   \n",
              "3  #model   i love u take with u all the time in ...   \n",
              "4             factsguide: society now    #motivation   \n",
              "\n",
              "                                          clean_text  \n",
              "0   user father dysfunctional selfish drags kids ...  \n",
              "1   user user thanks lyft credit can t use cause ...  \n",
              "2                                     bihday majesty  \n",
              "3                       model love u take u time ur   \n",
              "4                      factsguide society motivation  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b3e6f46f-db00-47e7-b1e7-1e5ce7eb0aa5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "      <td>user father dysfunctional selfish drags kids ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "      <td>user user thanks lyft credit can t use cause ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday majesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "      <td>model love u take u time ur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>factsguide society motivation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3e6f46f-db00-47e7-b1e7-1e5ce7eb0aa5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b3e6f46f-db00-47e7-b1e7-1e5ce7eb0aa5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b3e6f46f-db00-47e7-b1e7-1e5ce7eb0aa5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "FXstAbHySV9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['clean_text'])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNFh5QAWTbP-",
        "outputId": "0a3520a7-f68a-4b0c-9650-f57ae95a6479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39085"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max(len(data) for data in df['clean_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0itnNHVTbSo",
        "outputId": "f5f91e1f-94af-4fe6-dc20-84103671197d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# padding text data\n",
        "sequences = tokenizer.texts_to_sequences(df['clean_text'])\n",
        "padded_seq = pad_sequences(sequences, maxlen=131, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "E0xRnDA6TbUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seq[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS6tkMNJTinl",
        "outputId": "68222cdc-fb4f-4b8e-e3c5-38efc967a15a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    1,    28, 15330,  2630,  6365,   184,  7786,   385,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create embedding index\n",
        "embedding_index = {}\n",
        "with open('glove-model/glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs"
      ],
      "metadata": {
        "id": "2c0cSymbTivM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_index['good']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMguG8ffT-C1",
        "outputId": "d1538664-4a97-4f83-8aef-e70af8ff3678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.030769 ,  0.11993  ,  0.53909  , -0.43696  , -0.73937  ,\n",
              "       -0.15345  ,  0.081126 , -0.38559  , -0.68797  , -0.41632  ,\n",
              "       -0.13183  , -0.24922  ,  0.441    ,  0.085919 ,  0.20871  ,\n",
              "       -0.063582 ,  0.062228 , -0.051234 , -0.13398  ,  1.1418   ,\n",
              "        0.036526 ,  0.49029  , -0.24567  , -0.412    ,  0.12349  ,\n",
              "        0.41336  , -0.48397  , -0.54243  , -0.27787  , -0.26015  ,\n",
              "       -0.38485  ,  0.78656  ,  0.1023   , -0.20712  ,  0.40751  ,\n",
              "        0.32026  , -0.51052  ,  0.48362  , -0.0099498, -0.38685  ,\n",
              "        0.034975 , -0.167    ,  0.4237   , -0.54164  , -0.30323  ,\n",
              "       -0.36983  ,  0.082836 , -0.52538  , -0.064531 , -1.398    ,\n",
              "       -0.14873  , -0.35327  , -0.1118   ,  1.0912   ,  0.095864 ,\n",
              "       -2.8129   ,  0.45238  ,  0.46213  ,  1.6012   , -0.20837  ,\n",
              "       -0.27377  ,  0.71197  , -1.0754   , -0.046974 ,  0.67479  ,\n",
              "       -0.065839 ,  0.75824  ,  0.39405  ,  0.15507  , -0.64719  ,\n",
              "        0.32796  , -0.031748 ,  0.52899  , -0.43886  ,  0.67405  ,\n",
              "        0.42136  , -0.11981  , -0.21777  , -0.29756  , -0.1351   ,\n",
              "        0.59898  ,  0.46529  , -0.58258  , -0.02323  , -1.5442   ,\n",
              "        0.01901  , -0.015877 ,  0.024499 , -0.58017  , -0.67659  ,\n",
              "       -0.040379 , -0.44043  ,  0.083292 ,  0.20035  , -0.75499  ,\n",
              "        0.16918  , -0.26573  , -0.52878  ,  0.17584  ,  1.065    ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size+1, 100))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "K9Qlj-k9T-FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHJ1wihjT-Hf",
        "outputId": "7be1321e-8247-4808-bd44-4cf8433266fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39086, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2Vec"
      ],
      "metadata": {
        "id": "_66s0LDu-mqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYhFjFod_dsa",
        "outputId": "c620afe8-11a7-4fa6-fe9d-8c4d6d77f0b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrBiObL9_h_T",
        "outputId": "4038c134-9ea2-4bd0-f8d4-30fa3899f18d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import os"
      ],
      "metadata": {
        "id": "qtb9tClH_L8g"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gensim --user"
      ],
      "metadata": {
        "id": "lPduM4q6Demo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "story = []\n",
        "for filename in os.listdir('data'):\n",
        "    if filename == '.ipynb_checkpoints':\n",
        "      pass\n",
        "    f = open(os.path.join('data',filename))\n",
        "    corpus = f.read()\n",
        "    raw_sent = sent_tokenize(corpus)\n",
        "    for sent in raw_sent:\n",
        "        story.append(simple_preprocess(sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPrW0SoN_W2q",
        "outputId": "c7a3bd1f-f8d3-4819-9f46-b01f79abd63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(\n",
        "    window=10,\n",
        "    min_count=2\n",
        ")"
      ],
      "metadata": {
        "id": "EXvdA0KjCO4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.build_vocab(story)"
      ],
      "metadata": {
        "id": "uyEf0dk9Cbja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(story, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV5QuD4dCdOb",
        "outputId": "c2af7c96-6b9a-45b8-a202-ecbf912cfc2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(322593, 447775)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('daenerys')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX_APj3UCfFy",
        "outputId": "e8a78038-e66b-47b9-856a-c17cea31cc7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('still', 0.9991421103477478),\n",
              " ('an', 0.9991043210029602),\n",
              " ('dothraki', 0.9990985989570618),\n",
              " ('other', 0.9990887641906738),\n",
              " ('illyrio', 0.9990878105163574),\n",
              " ('three', 0.9990662932395935),\n",
              " ('own', 0.999061107635498),\n",
              " ('half', 0.9990563988685608),\n",
              " ('above', 0.9990508556365967),\n",
              " ('prince', 0.9990479946136475)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.wv.doesnt_match(['jon','rikon','robb','arya','sansa','bran'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9h47s2ccCh1S",
        "outputId": "60ed50df-5081-4fa8-af13-50867dab0f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.keyedvectors:vectors for words {'rikon'} are not present in the model, ignoring these words\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'arya'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.similarity('arya','sansa')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYX24SHWClIi",
        "outputId": "3f1c6322-6f35-4a6b-d21b-a7ea489f3d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.999732"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.get_normed_vectors()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAOJ35vmDU-C",
        "outputId": "1176afb5-f7a5-4ed3-8134-0814e4b9cec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.14384648,  0.06898983,  0.09060675, ..., -0.09030579,\n",
              "         0.08961223,  0.03053357],\n",
              "       [-0.14251111,  0.06770036,  0.08857375, ..., -0.08862758,\n",
              "         0.08707452,  0.02521757],\n",
              "       [-0.10342573,  0.05004968,  0.07299528, ..., -0.08659401,\n",
              "         0.08864916, -0.00809827],\n",
              "       ...,\n",
              "       [-0.08865785,  0.10819329,  0.0983897 , ..., -0.10497634,\n",
              "         0.08924235,  0.02983036],\n",
              "       [-0.09105065,  0.07982258,  0.0453524 , ..., -0.11258623,\n",
              "         0.06816661, -0.02449272],\n",
              "       [-0.03026164,  0.11775769, -0.00269076, ..., -0.07278508,\n",
              "         0.09833761, -0.01210407]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.get_normed_vectors().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOQlWPBODXrk",
        "outputId": "3c6ed4e0-2f47-44d3-fd52-7cbaf20f1445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3840, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y = model.wv.index_to_key"
      ],
      "metadata": {
        "id": "qb2xGZsXC9JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "rpX0bPdOCpeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pca = PCA(n_components=3)"
      ],
      "metadata": {
        "id": "4NFJlN3aCrqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = pca.fit_transform(model.wv.get_normed_vectors())"
      ],
      "metadata": {
        "id": "wh4e5ej5C0_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgQROFJoC2Oy",
        "outputId": "96914944-9f75-42ca-9f5a-88d798071ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.0191739 , -0.18675189, -0.00540263],\n",
              "       [-0.02404007, -0.16965686, -0.00258641],\n",
              "       [-0.05078571,  0.01602564,  0.00224132],\n",
              "       ...,\n",
              "       [ 0.00945821,  0.03468359, -0.0463189 ],\n",
              "       [-0.02006017, -0.04859893, -0.01771331],\n",
              "       [ 0.03893204,  0.06801885,  0.06203329]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPNmneT7DQeJ",
        "outputId": "ec05b689-0bd1-499b-b9e6-3cf53dbf40c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3840, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "fig = px.scatter_3d(X[200:300],x=0,y=1,z=2, color=y[200:300])\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "yh46pgKSDjOZ",
        "outputId": "b2bd359c-a241-4580-c6ed-09466ede53db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"35e1de63-7a2f-4a3f-8422-a75276149aba\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"35e1de63-7a2f-4a3f-8422-a75276149aba\")) {                    Plotly.newPlot(                        \"35e1de63-7a2f-4a3f-8422-a75276149aba\",                        [{\"hovertemplate\":\"color=put<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"put\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"put\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.049547430127859116],\"y\":[-0.008693198673427105],\"z\":[0.0007840798934921622],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=take<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"take\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"take\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05217323079705238],\"y\":[0.053557898849248886],\"z\":[0.0011053169146180153],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=words<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"words\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"words\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.050149042159318924],\"y\":[-0.001666689058765769],\"z\":[-0.0018258483614772558],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=keep<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"keep\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"keep\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.051442649215459824],\"y\":[0.015750057995319366],\"z\":[0.001547583844512701],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=gods<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"gods\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"gods\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05174664407968521],\"y\":[0.041915882378816605],\"z\":[0.003986176569014788],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=grey<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"grey\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"grey\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04759006202220917],\"y\":[-0.0343126505613327],\"z\":[-0.0017320476472377777],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=hear<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"hear\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"hear\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05167108401656151],\"y\":[0.059140391647815704],\"z\":[0.00400018785148859],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=name<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"name\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"name\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05078953877091408],\"y\":[0.0186237134039402],\"z\":[-0.002096050651744008],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=should<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"should\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"should\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05218612402677536],\"y\":[0.054661545902490616],\"z\":[0.004442856181412935],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=might<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"might\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"might\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.051948100328445435],\"y\":[0.06338980793952942],\"z\":[0.0023661262821406126],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=always<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"always\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"always\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05143902450799942],\"y\":[0.027151774615049362],\"z\":[0.001202559331431985],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=red<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"red\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"red\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.046240027993917465],\"y\":[-0.047089263796806335],\"z\":[-0.001257635303772986],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=children<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"children\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"children\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.050309501588344574],\"y\":[0.0017569641349837184],\"z\":[-0.0004656360833905637],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=north<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"north\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"north\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04846111312508583],\"y\":[-0.01898937113583088],\"z\":[-0.00032790008117444813],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=our<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"our\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"our\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.051743410527706146],\"y\":[0.04089231416583061],\"z\":[0.00016912198043428361],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=mother<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"mother\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"mother\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05141796916723251],\"y\":[0.04058848321437836],\"z\":[-0.0052523002959787846],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=enough<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"enough\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"enough\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.051788169890642166],\"y\":[0.05208107829093933],\"z\":[-0.002994316164404154],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=feet<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"feet\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"feet\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.044294655323028564],\"y\":[-0.05790457874536514],\"z\":[-0.004084121901541948],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=once<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"once\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"once\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05074099078774452],\"y\":[0.02344430983066559],\"z\":[-9.281784150516614e-05],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=perhaps<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"perhaps\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"perhaps\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.051156576722860336],\"y\":[0.03859184682369232],\"z\":[-0.001953311963006854],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=make<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"make\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"make\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05116407200694084],\"y\":[0.056397322565317154],\"z\":[-0.0019602742977440357],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=littlefinger<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"littlefinger\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"littlefinger\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.050700969994068146],\"y\":[0.015876995399594307],\"z\":[0.0010579053778201342],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=castle<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"castle\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"castle\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.047559160739183426],\"y\":[-0.02662297897040844],\"z\":[-0.001167438691481948],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=small<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"small\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"small\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.049592819064855576],\"y\":[-0.00595472939312458],\"z\":[-0.0018575018038973212],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=under<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"under\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"under\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.0477924607694149],\"y\":[-0.030434221029281616],\"z\":[-0.001333600957877934],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=woman<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"woman\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"woman\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05038350075483322],\"y\":[0.007478668820112944],\"z\":[0.0015479755820706487],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=stone<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"stone\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"stone\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04877793416380882],\"y\":[-0.02415764518082142],\"z\":[-0.0013463671784847975],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=son<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"son\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"son\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05098637938499451],\"y\":[0.02805192768573761],\"z\":[0.002770697697997093],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=high<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"high\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"high\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04798710718750954],\"y\":[-0.026551498100161552],\"z\":[0.0006429190980270505],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=seen<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"seen\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"seen\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05072224140167236],\"y\":[0.03229476138949394],\"z\":[-0.0011904110433533788],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=horse<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"horse\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"horse\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04921602085232735],\"y\":[-0.015184612944722176],\"z\":[-0.004230596590787172],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=arms<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"arms\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"arms\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04565229266881943],\"y\":[-0.05032770708203316],\"z\":[-0.0003103477065451443],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=against<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"against\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"against\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04851992800831795],\"y\":[-0.026127612218260765],\"z\":[7.595307397423312e-05],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=moment<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"moment\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"moment\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05028713867068291],\"y\":[0.008231235668063164],\"z\":[3.40470032824669e-05],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=between<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"between\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"between\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.047762300819158554],\"y\":[-0.02792447619140148],\"z\":[-0.001614537090063095],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=dark<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"dark\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"dark\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04772630333900452],\"y\":[-0.027629928663372993],\"z\":[-0.004853354301303625],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=dothraki<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"dothraki\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"dothraki\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04873618483543396],\"y\":[-0.011641076765954494],\"z\":[-0.003872977104038],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=pulled<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"pulled\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"pulled\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.041715964674949646],\"y\":[-0.08082852512598038],\"z\":[-0.003022815566509962],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=place<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"place\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"place\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.051798369735479355],\"y\":[0.036106180399656296],\"z\":[-0.0010743149323388934],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=three<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"three\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"three\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04949650168418884],\"y\":[-0.011105193756520748],\"z\":[-0.0003972893755417317],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=window<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"window\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"window\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.045843660831451416],\"y\":[-0.043719325214624405],\"z\":[-0.0012383920839056373],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=very<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"very\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"very\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.0520235113799572],\"y\":[0.038712743669748306],\"z\":[0.003756666788831353],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=find<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"find\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"find\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05128907784819603],\"y\":[0.05309685319662094],\"z\":[-0.002903863787651062],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=gone<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"gone\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"gone\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04890940338373184],\"y\":[-0.013152366504073143],\"z\":[-0.0015469768550246954],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=drogo<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"drogo\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"drogo\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.049443770200014114],\"y\":[-0.007929484359920025],\"z\":[-0.0038856372702866793],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=fire<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"fire\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"fire\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04843124374747276],\"y\":[-0.021153222769498825],\"z\":[-0.0025539083871990442],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=began<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"began\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"began\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.0484149307012558],\"y\":[-0.02321525104343891],\"z\":[-0.002509302692487836],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=these<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"these\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"these\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.051573850214481354],\"y\":[0.053798627108335495],\"z\":[-0.0033061618451029062],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=seven<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"seven\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"seven\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04982758313417435],\"y\":[-0.0024630504194647074],\"z\":[-0.0018509910441935062],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=eddard<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"eddard\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"eddard\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.051674649119377136],\"y\":[0.03536062315106392],\"z\":[-0.0022997104097157717],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=yes<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"yes\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"yes\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05224783346056938],\"y\":[0.03512902557849884],\"z\":[0.005200927145779133],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=dragon<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"dragon\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"dragon\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05045489966869354],\"y\":[-0.0008960036793723702],\"z\":[0.001216792967170477],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=sat<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"sat\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"sat\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04412555322051048],\"y\":[-0.06177247688174248],\"z\":[-0.000834057864267379],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=rodrik<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"rodrik\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"rodrik\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04922711104154587],\"y\":[0.009133035317063332],\"z\":[-0.002479497343301773],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=septa<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"septa\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"septa\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05055227875709534],\"y\":[0.019276119768619537],\"z\":[-1.9424960555625148e-05],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=life<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"life\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"life\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04961560666561127],\"y\":[-0.0021901705767959356],\"z\":[-0.0018607276724651456],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=khal<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"khal\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"khal\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.0491572804749012],\"y\":[-0.006637135054916143],\"z\":[0.00744215352460742],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=another<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"another\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"another\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05010348930954933],\"y\":[0.0061151133850216866],\"z\":[0.0012784950667992234],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=sweet<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"sweet\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"sweet\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.051068589091300964],\"y\":[0.027920860797166824],\"z\":[-0.0021857628598809242],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=tower<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"tower\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"tower\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04805663600564003],\"y\":[-0.02519606612622738],\"z\":[0.00014125253073871136],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=any<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"any\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"any\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05185751989483833],\"y\":[0.052450209856033325],\"z\":[-0.0011907675070688128],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=smile<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"smile\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"smile\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.048861000686883926],\"y\":[-0.016894742846488953],\"z\":[-0.0022149488795548677],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=wind<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"wind\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"wind\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04698384925723076],\"y\":[-0.031944915652275085],\"z\":[-0.0033835098147392273],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=something<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"something\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"something\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.051866769790649414],\"y\":[0.033900633454322815],\"z\":[0.004207514226436615],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=every<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"every\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"every\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05022887513041496],\"y\":[-0.0009476030827499926],\"z\":[0.0014105505542829633],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=ride<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"ride\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"ride\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.0508076436817646],\"y\":[0.021760636940598488],\"z\":[-0.0007150674937292933],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=viserys<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"viserys\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"viserys\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04988684505224228],\"y\":[-0.0002080700360238552],\"z\":[-0.00032773593557067215],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=girl<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"girl\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"girl\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.050779763609170914],\"y\":[0.012819534167647362],\"z\":[0.0028437438886612654],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=end<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"end\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"end\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.048544298857450485],\"y\":[-0.01959269680082798],\"z\":[0.0008766613900661469],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=beneath<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"beneath\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"beneath\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.0476582907140255],\"y\":[-0.02845028229057789],\"z\":[0.00011652108514681458],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=things<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"things\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"things\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.051992855966091156],\"y\":[0.04386909678578377],\"z\":[0.0008879613014869392],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=afraid<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"afraid\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"afraid\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05142854526638985],\"y\":[0.052359890192747116],\"z\":[-0.002543011913076043],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=both<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"both\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"both\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04847794771194458],\"y\":[-0.020189275965094566],\"z\":[0.0006988564855419099],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=far<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"far\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"far\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04969422146677971],\"y\":[-0.0007041224744170904],\"z\":[-0.000802656402811408],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=side<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"side\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"side\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.046413175761699677],\"y\":[-0.0385931171476841],\"z\":[-0.002122933277860284],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=silver<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"silver\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"silver\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.047524865716695786],\"y\":[-0.03389820456504822],\"z\":[0.001165164983831346],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=re<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"re\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"re\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05106636509299278],\"y\":[0.10958197712898254],\"z\":[0.007407024502754211],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=those<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"those\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"those\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.049556247889995575],\"y\":[-0.00416919868439436],\"z\":[-0.002322609070688486],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=though<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"though\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"though\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.049711886793375015],\"y\":[0.00894054863601923],\"z\":[-0.0037959450855851173],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=get<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"get\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"get\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05065852776169777],\"y\":[0.02398659661412239],\"z\":[-0.0034450984094291925],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=summer<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"summer\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"summer\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04928436875343323],\"y\":[-0.004223128315061331],\"z\":[0.0012231905711814761],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=done<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"done\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"done\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.050696663558483124],\"y\":[0.05065597966313362],\"z\":[0.011750315316021442],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=young<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"young\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"young\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05067514255642891],\"y\":[0.011108300648629665],\"z\":[0.002475415589287877],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=others<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"others\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"others\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04893383011221886],\"y\":[-0.009233260527253151],\"z\":[-0.0013090723659843206],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=need<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"need\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"need\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05176234245300293],\"y\":[0.09076846390962601],\"z\":[0.004575333558022976],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=feel<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"feel\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"feel\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04956286773085594],\"y\":[0.010508391074836254],\"z\":[-0.00584642868489027],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=most<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"most\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"most\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05032685399055481],\"y\":[0.014983098953962326],\"z\":[0.006588521413505077],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=smiled<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"smiled\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"smiled\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04880271479487419],\"y\":[-0.017278030514717102],\"z\":[-0.0007354254485107958],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=brothers<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"brothers\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"brothers\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.049011774361133575],\"y\":[-0.004389198962599039],\"z\":[-0.0016602312680333853],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=table<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"table\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"table\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04617772251367569],\"y\":[-0.04066585749387741],\"z\":[-0.0021324632689356804],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=right<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"right\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"right\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04750794544816017],\"y\":[-0.029426509514451027],\"z\":[-0.0028022280894219875],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=thing<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"thing\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"thing\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.051676828414201736],\"y\":[0.05524580180644989],\"z\":[0.0014736150624230504],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=honor<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"honor\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"honor\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05163692310452461],\"y\":[0.03248657286167145],\"z\":[0.004427920561283827],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=taken<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"taken\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"taken\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04820740595459938],\"y\":[-0.011944307014346123],\"z\":[0.0016765673644840717],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=wanted<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"wanted\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"wanted\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.051037438213825226],\"y\":[0.038298267871141434],\"z\":[0.0016963161760941148],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=few<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"few\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"few\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.05096821114420891],\"y\":[0.018210412934422493],\"z\":[0.0019369055517017841],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=laughed<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"laughed\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"laughed\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04931657388806343],\"y\":[-0.008137171156704426],\"z\":[0.005733184982091188],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=legs<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"legs\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"legs\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.0486295185983181],\"y\":[-0.02418104000389576],\"z\":[-0.0028044835198670626],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=almost<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"almost\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"almost\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.04837445542216301],\"y\":[-0.015337706543505192],\"z\":[0.001272236811928451],\"type\":\"scatter3d\"},{\"hovertemplate\":\"color=bastard<br>0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>\",\"legendgroup\":\"bastard\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bastard\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.051950469613075256],\"y\":[0.05101434513926506],\"z\":[-0.002474272157996893],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"0\"}},\"yaxis\":{\"title\":{\"text\":\"1\"}},\"zaxis\":{\"title\":{\"text\":\"2\"}}},\"legend\":{\"title\":{\"text\":\"color\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('35e1de63-7a2f-4a3f-8422-a75276149aba');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wag7ZHA0Dkui"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q_9w0BPMyuL4",
        "P53EmPynyuL5",
        "0n2YUwi1yuL-",
        "tGd8-mdVyuMA",
        "n3sOgkIoyuMB",
        "CXNYTNgYyuMI",
        "xdPjrowYyuMb",
        "U8dpWgzryuMb",
        "K-ye2OG2yuMe",
        "cFklgTn9yuMi",
        "0YarWzEPyuMs"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}